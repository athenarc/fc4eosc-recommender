{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import bottleneck as bn\n",
    "from hyperopt import hp\n",
    "from recpack.pipelines import PipelineBuilder, HyperoptInfo\n",
    "from recpack.pipelines.registries import ALGORITHM_REGISTRY\n",
    "from recpack.preprocessing.filters import NMostPopular\n",
    "from recpack.preprocessing.preprocessors import DataFramePreprocessor\n",
    "from recpack.scenarios import StrongGeneralization\n",
    "\n",
    "from darelabdb.recs_neighborhood_learning.ease import myEASE\n",
    "from darelabdb.utils_database_connector.core import Database\n",
    "from darelabdb.api_faircore_neighborhood_learning_recs.db.rec_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Database(\"fc4eosc\")\n",
    "\n",
    "communities = {\"beopen\", \"dh-ch\", \"enermaps\", \"eosc\", \"dariah\"}\n",
    "\n",
    "base_directory = \"communities\"\n",
    "\n",
    "ALGORITHM_REGISTRY.register(myEASE.__name__, myEASE) # register if not already registered\n",
    "\n",
    "# Define time threshold and number of evaluations\n",
    "SECONDS = 12*3600; EVALUATIONS = 25\n",
    "\n",
    "BATCH_SIZE = 20000 # batch size for getting recs\n",
    "\n",
    "# Define recommendation list sizes for metrics\n",
    "K = [10, 20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topn_indices(R_hat, n):\n",
    "    \"\"\"\n",
    "    Helper function to get sorted indices of top-n items in each row of R_hat.\n",
    "    \"\"\"\n",
    "    users = R_hat.shape[0]\n",
    "    \n",
    "    # find the indices that partition the array so that the first n elements are the largest n elements\n",
    "    idx_topn_part = bn.argpartition(-R_hat, n, axis=1)\n",
    "\n",
    "    # keep only the largest n elements of R_hat\n",
    "    topn_part = R_hat[np.arange(users)[:, np.newaxis], idx_topn_part[:, :n]]\n",
    "\n",
    "    # find the indeces of the sorted top-n predicted relevance scores in R_hat\n",
    "    idx_part = np.argsort(-topn_part, axis=1)\n",
    "    idx_topn = idx_topn_part[np.arange(users)[:, np.newaxis], idx_part]\n",
    "    \n",
    "    return idx_topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for community in communities:\n",
    "\n",
    "    # Collect data\n",
    "    df = get_citations_by_community(db, community)\n",
    "    if df.empty: print('DataFrame is empty!')\n",
    "\n",
    "    df_pp = DataFramePreprocessor(\"result_id\", \"author_id\")  # define preprocessor\n",
    "\n",
    "    pop_limit = min(df[\"result_id\"].nunique(), int(1e5)) # conditionally set the limit for NMostPopular filter\n",
    "\n",
    "    # Define filters\n",
    "    n_most_popular_filter = NMostPopular(pop_limit, \"result_id\")\n",
    "\n",
    "    # Apply filters\n",
    "    df_pp.add_filter(n_most_popular_filter)\n",
    "\n",
    "    # Create interaction matrix object\n",
    "    im = df_pp.process(df)\n",
    "\n",
    "    scenario = StrongGeneralization(validation=True, seed=1452)\n",
    "    scenario.split(im)\n",
    "\n",
    "    # Set optimisation details\n",
    "    optimisation_info_ease = HyperoptInfo(\n",
    "        {\"l2\": hp.loguniform(\"l2\", np.log(1e0), np.log(1e4))},\n",
    "        timeout = SECONDS,\n",
    "        max_evals = EVALUATIONS,\n",
    "    )\n",
    "\n",
    "    results_folder = os.path.join(base_directory, f\"{community}-results\")\n",
    "    if not os.path.exists(results_folder): os.makedirs(results_folder)\n",
    "    \n",
    "    # Start pipeline for fine-tuning\n",
    "    pb = PipelineBuilder(folder_name=results_folder)\n",
    "    pb.set_data_from_scenario(scenario)\n",
    "    pb.add_algorithm(\"myEASE\", optimisation_info=optimisation_info_ease, params={\"method\": \"item\"})\n",
    "    pb.set_optimisation_metric(\"NDCGK\", 10)\n",
    "    pb.add_metric(\"NDCGK\", K)\n",
    "    pb.add_metric(\"RecallK\", K)\n",
    "    pb.add_metric(\"CoverageK\", K)\n",
    "\n",
    "    pipe = pb.build()\n",
    "    pipe.run()\n",
    "    pipe.save_metrics() # save results\n",
    "\n",
    "    # Training model with optimal parameters\n",
    "    opt_results = pipe.optimisation_results\n",
    "    myEASE_rows = opt_results[opt_results[\"algorithm\"] == \"myEASE\"]\n",
    "    opt_myEASE_row = myEASE_rows.loc[myEASE_rows[\"NDCGK_\" + str(10)].idxmax()]\n",
    "    ease_params = {\"l2\": opt_myEASE_row[\"params\"][\"l2\"]}\n",
    "\n",
    "    num_users = im.values.shape[0]\n",
    "\n",
    "    model = myEASE(ease_params[\"l2\"], method=\"item\")\n",
    "    model.fit(im)\n",
    "\n",
    "    # Process recommendations in batches\n",
    "    for start_index in range(0, num_users, BATCH_SIZE):\n",
    "        end_index = min(start_index + BATCH_SIZE, num_users)\n",
    "        batch_data = im.values[start_index:end_index, :]\n",
    "        author_ids_batch = df_pp.user_id_mapping['author_id'][start_index:end_index].tolist()\n",
    "\n",
    "        # Get recommendations for the current batch\n",
    "        predictions = model.predict(batch_data).toarray()\n",
    "        topn_lists = get_topn_indices(predictions, 20)\n",
    "        topn_lists_real_ids = [[df_pp.item_id_mapping[\"result_id\"][idx] for idx in topn] for topn in topn_lists]\n",
    "\n",
    "        data_tuples = prepare_recommendation_data(author_ids_batch, topn_lists_real_ids, community)\n",
    "        write_recommendations(db, data_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "\n",
    "# # Get available memory in MB\n",
    "# available_memory_MB = psutil.virtual_memory().available / (1024 * 1024)\n",
    "\n",
    "# # Assume we want to limit the usage to 50% of available memory\n",
    "# max_memory_usage_MB = available_memory_MB * 0.5\n",
    "\n",
    "# # Calculate the maximum number of items we can handle given both P and its inverse (EASE recommender)\n",
    "# max_items = int(np.sqrt((max_memory_usage_MB * 1024**2) / (2 * 8)))\n",
    "\n",
    "# print(f\"Maximum number of items accounting for inversion: {max_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for community in communities:\n",
    "\n",
    "#     # Collect data\n",
    "#     df = get_citations_by_community(db, community)\n",
    "#     if df.empty:\n",
    "#         print(f'Community {community}: DataFrame is empty!')\n",
    "#         continue  # Skip to the next community if DataFrame is empty\n",
    "\n",
    "#     # Print the number of authors and results before preprocessing\n",
    "#     num_authors_before = df[\"author_id\"].nunique()\n",
    "#     num_results_before = df[\"result_id\"].nunique()\n",
    "#     print(f'Community {community} - Before Preprocessing: {num_authors_before} authors, {num_results_before} results')\n",
    "\n",
    "#     # Define preprocessor\n",
    "#     df_pp = DataFramePreprocessor(\"result_id\", \"author_id\")\n",
    "\n",
    "#     # Conditionally set the limit for NMostPopular filter\n",
    "#     pop_limit = min(df[\"result_id\"].nunique(), int(1e5))\n",
    "\n",
    "#     # Define filters\n",
    "#     n_most_popular_filter = NMostPopular(pop_limit, \"result_id\")\n",
    "\n",
    "#     # Apply filters\n",
    "#     df_pp.add_filter(n_most_popular_filter)\n",
    "\n",
    "#     # Create interaction matrix object\n",
    "#     im = df_pp.process(df)\n",
    "\n",
    "#     # Print the number of authors and results after preprocessing\n",
    "#     num_authors_after = im.shape[0]\n",
    "#     num_results_after = im.shape[1]\n",
    "#     print(f'Community {community} - After Preprocessing: {num_authors_after} authors, {num_results_after} results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
